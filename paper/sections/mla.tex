Deepseek-V3 MLA implements a lower rank key value (KV) cache as opposed to our original MHA in which each query vector attends to a corresponding KV vector\cite{liu2024deepseekv2}. Standard approach to MLA reduces both the query and KV space to a lower-dimension latent space with learnable latent vectors\cite{liu2024deepseekv2}. Our model adopts a hybrid approach, wherein only the query vectors are projected into the latent space, while the KV vectors remain in the original input space. We initialize learnable latent tokens using torch.nn.Parameter, and linearly project these tokens into the query space through torch.nn.Linear during the forward pass of each attention head. In the absence of externally provided latent vectors, the model expands a fixed set of latent tokens to match the batch size. We employ a query cache that stores pre-computed query representations derived from the projected latent vectors within each latent attention head.