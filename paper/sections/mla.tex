Deepseek-V3 MLA implements a lower rank key value (KV) cache as opposed to our original MHA in which each query vector attends to a corresponding KV vector\cite{liu2024deepseekv2}. Standard approach to MLA reduces both the query and KV space to a lower-dimension latent space with learnable latent vectors\cite{liu2024deepseekv2}. Our model takes a hybrid approach that only projects query vectors to lower-dimension latent space. We first randomly generate learnable latent tokens using torch.nn.Parameter. We then linearly transform the latent tokens to the query space using torch.nn.Linear in the forward pass of each attention head. If no latent vectors are passed in, we expand the original latents to match batch size. We utilizie a query cache to store pre-computed query vectors derived from latent query vectors in each latent attention head.
\subsection{Custom MLA}