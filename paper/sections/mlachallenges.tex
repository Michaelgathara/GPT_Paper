Empirical results suggest that the MLA architecture achieves improved optimization performance when compared to the our baseline MHA model, as evidenced by lower final loss values after 5,000 training iterations. However, qualitative evaluation of generated outputs by each model when given prompts indicates that the MHA model produces more coherent and human-readable responses when compared with the repetitive 1 word responses produced by the MLA model. We presume 2 potential reasons why this may be the case. \newline\newline First, we utilize a learnable absolute positional embeddings(APE) in our MLA model. APEs are added to the input embedding space before projection to latent space. This leads to a few issues: 1. Query vectors are projected to a latent space, meaning positional information may be lost or distorted. 2. APE only encodes absolute position, meaning the model has no relative positional awareness. This is especially important in MLA, which projects queries to a fixed-size latent space\cite{vaswani2023attentionneed}\cite{su2023rope}. 3. APE does not preserve relative meaning when sequences are shifted, such as during latent projection in MLA. As such, we propose the next step to be utilizing a decoupled Rotary Position Embedding (RoPE), as implemented by Deepseek-V2\cite{liu2024deepseekv2}. While traditional RoPE rotates both query and key vectors to encode relative position\cite{su2023rope}, the proposed decoupled RoPE would only apply to the query vector due to its projection to latent space\cite{liu2024deepseekv2} before computing attention. \newline\newline Second, due to resource constraints, our custom BPE tokenizer significantly reduces the number of tokens in our model. This, in combination with a reduced number of learnable latent vectors attending to the KV space, may have resulted in the model overfitting to specific input tokens. We propose utilizing a significantly larger dataset for training with more tokens and increased latent vectors may reduce overfitting, thus leading to better qualitative output in our hybrid MLA approach. 