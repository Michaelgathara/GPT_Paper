In their seminal 2017 paper, Vaswani et al. introduced the 
transformer architecture and set the stage for the many iterations of LLMs that 
have come in the years since \cite{vaswani2023attentionneed}. This original paper focused on the core attention 
component of the transformer which calculates the interactions between different
input tokens and uses this contextual knowledge to more dynamically generate 
relevant output text.