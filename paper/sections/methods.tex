We present a transformer-based language model that employs a novel attention mechanism called Multi-headed Latent Attention (MLA). This section details the architecture of our model and the training methodology.
\subsection{Model Architecture}
Our model follows the general transformer architecture~\cite{vaswani2017attention} with several key modifications. The main model consists of 12 transformer layers, each containing a multiheaded latent attention module and a feed-forward network. We use an embedding dimension of 768 and 12 attention heads throughout the model. The context window is limited to 512 tokens.
The input token sequence is first embedded using a token embedding layer and combined with learned positional embeddings:
\begin{equation}
h_0 = W_e \cdot x + W_p
\end{equation}
\noindent where $W_e \in \mathbb{R}^{V \times d}$ is the token embedding matrix for vocabulary size $V$ and embedding dimension $d$, and $W_p \in \mathbb{R}^{n \times d}$ is the positional embedding matrix for a maximum sequence length $n$.
\subsubsection{Multi-headed Latent Attention (MLA)}
\input{sections/mla.tex}
\subsubsection{Feed-Forward Networks}
After the attention layer, each transformer block contains a feed-forward network using SwiGLU activation~\cite{shazeer2020glu}, which has been shown to outperform ReLU in language models. The feed-forward network is defined as:
\begin{align}
\text{Swish}(xW_1) \otimes (xW_2) \
\text{FFN}(x) = \text{SwiGLU}(x)W_3
\end{align}
\noindent where $W_1, W_2 \in \mathbb{R}^{d \times 4d}$ and $W_3 \in \mathbb{R}^{4d \times d}$ are learned parameter matrices, $\text{Swish}(x) = x \cdot \sigma(x)$ is the swish activation function, and $\otimes$ represents element-wise multiplication.
Each attention and feed-forward sublayer is wrapped with a residual connection and layer normalization:
\begin{align}
  h'_i &= \mathrm{LN}(h_i) \\
  h_{i+\frac12} &= h_i + \mathrm{MLA}(h'_i) \\
  h''_{i+\frac12} &= \mathrm{LN}(h_{i+\frac12}) \\
  h_{i+1} &= h_{i+\frac12} + \mathrm{FFN}(h''_{i+\frac12})
\end{align}
\noindent where $\text{LN}$ denotes layer normalization.
\subsubsection{Custom Byte Pair Encoding}
\paragraph{Configuration and Hyperparameters}
We train a custom Byte-Pair Encoding (BPE) tokenizer~\cite{sennrich2016neural} on the Wikitext-103 corpus with a vocabulary size of 25,000. The tokenizer includes special tokens \texttt{[PAD]}, \texttt{[UNK]}, \texttt{[CLS]}, \texttt{[SEP]}, \texttt{[MASK]}, \texttt{[BOS]}, and \texttt{[EOS]}. We use whitespace pre-tokenization and set a minimum token frequency of 2 to filter out rare tokens.
\paragraph{Data Preprocessing}
The raw Wikitext-103 training split is first cleaned to remove markup and normalize whitespace. To process the corpus efficiently, we perform the cleaning in grouped batches using multiprocessing. Each cleaned line is then split on whitespace to produce an initial sequence of byte tokens for BPE training. Our cleaning routine leverages pre-compiles regular expressions to strip wiki-specific formatting, correct hyphenation and punctuation spacing, and collapse multiple spaces. The resulting cleaned lines are saved as a newline-delimited file on disk, providing a stable noise-reduced input for the BPE trainer.
\paragraph{Tokenizer Construction}
We instantiate a \texttt{Token\allowbreak izer} with a BPE model using \texttt{unk\_token="[UNK]"} to represent out-of-vocabulary subwords. A \texttt{Whitespace} pre-tokenizer is attached to split the cleaned text into initial byte tokens. We then configure a \texttt{BpeTrainer} with our target vocabulary size (25\,000), minimum merge frequency (2), and the reserved special tokens. The trainer drives the merge process, repeatedly identifying and merging the most frequent adjacent byte pairs until the vocabulary size is reached.
\paragraph{Tokenizer Training and Implementation}
BPE training is performed in a single pass over the cleaned text, merging byte-pairs until the vocabulary criterion is met. The resulting vocabulary and merge rules are then saved as a JSON file for reproducible tokenization. The system is implemented with the Rust-optimized core of the Hugging Face Tokenizers library, which parallelizes intensive steps across CPU cores, and uses buffered I/O to efficiently manage large files.
\paragraph{Tokenizer Loading and Consistency}
At inference, we load the JSON file produced by training; if it's missing, the cleaning and training pipeline is re-invoked automatically. This guarantees that both training and evaluation share an identical subword mapping.
\subsection{Training}
\subsubsection{Dataset}
We train our model on the Wikitext-103 corpus~\cite{merity2016pointer}, a collection of good quality Wikipedia articles containing approximately 1.8 million tokens. Prior to tokenization, we apply a text cleaning procedure to remove markup elements and normalize whitespace. The cleaned text is then tokenized and chunked into sequences of 512 tokens for training.
\subsubsection{Training Process}
The model is trained using distributed data parallelism across multiple NVIDIA H100 GPUs. We employ mixed precision training (FP16) with gradient scaling to improve computational efficiency while maintaining numerical stability. The training process uses a batch size of 72 per GPU with gradient accumulation over 4 steps, resulting in an effective batch size of 288.
We implement model parallelism using PyTorch's DistributedDataParallel (DDP) framework. The dataset is partitioned across GPUs using a DistributedSampler to ensure each worker processes different data. The training loop includes the following steps:
\begin{enumerate}
    \item Sample a batch $(x, y)$ from the data loader
    \item Perform a forward pass with mixed precision: $\text{logits}, \text{loss} = \text{model}(x, y)$
    \item Scale the loss
    \item Perform a backward pass with gradient scaling
    \item If $i \bmod \text{accumulation\_steps} = 0$:
    \begin{enumerate}
        \item Clip gradients to maximum norm 1.0
        \item Update model parameters
        \item Zero gradients
        \item Update learning rate with scheduler
    \end{enumerate}
    \item If $i \bmod \text{eval\_interval} = 0$:
    \begin{enumerate}
        \item Evaluate on validation set
        \item Save checkpoint if validation loss improved
    \end{enumerate}
\end{enumerate}
\noindent This process continues for a maximum of 15,000 iterations or until convergence.
\subsubsection{Optimization}
We optimize our model using AdamW~\cite{loshchilov2019decoupled} with a weight decay of $1 \times 10^{-4}$ and beta parameters $\beta_1 = 0.9$, $\beta_2 = 0.95$. The initial learning rate is set to $1 \times 10^{-3}$.
Our learning rate schedule combines a linear warmup phase with a cosine decay phase:
\begin{equation}
\text{lr}(t) =
\begin{cases}
\text{lr}_{\text{max}} \cdot \frac{t}{\text{warmup}} & \text{if } t < \text{warmup} \\
\ldots
\end{cases}
\end{equation}
\noindent where $\text{warmup} = 500$ iterations and $\text{max} = 15,000$ iterations. This schedule helps stabilize early training and gradually reduces the learning rate to achieve better convergence.
\noindent where \textit{warmup steps} eq 500 and \textit{max steps} eq 15,000. This schedule helps stabilize early training and gradually reduces the learning rate to achieve better convergence.
To prevent gradient explosion, we apply gradient clipping with a maximum norm of 1.0 before each optimizer step.
\subsubsection{Hyperparameter Tuning}
\input{sections/hyperparameter}
\subsubsection{Evaluation and Checkpointing}
We evaluate the model on the validation set every 100 iterations by computing the average loss over multiple batches. The best model is selected based on the lowest validation loss achieved during training. Additionally, we save periodic checkpoints every 500 iterations to enable resumption of training if needed.
To monitor training progress, we log key metrics including training loss, validation loss, learning rate, and throughput (tokens processed per second) using TensorBoard. After training, we generate sample text from the best checkpoint to qualitatively assess the model's capabilities.
\subsection{Testing}
For the testing of our model, we used two different primary metrics: loss and perplexity. The loss function that we utilized both in our model training as 
well as in the testing is the standard cross-entropy loss i.e. 
\begin{equation}
\mathcal{L}_{\text{CE}}
  \;=\;
  -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}
  y_{i,c}\;\log p_{i,c},
\end{equation}
where $y_{i,c} \in \{0,1\}$ is the one-hot reference label for example $i$ and class $c$, and $p_{i,c}$ is the model's predicted probability for that class.
And the perplexity is defined as:
\begin{equation}
\operatorname{PPL}
  \;=\;
  \exp\!\bigl(
      -\tfrac{1}{T}\sum_{t=1}^{T} \log p(w_t \mid w_{<t})
    \bigr),
\end{equation}
where $p(w_t \mid w_{<t})$ is the modelâ€™s predicted probability of token $w_t$ given its history $w_{<t}$.
In addition to these two metrics, we also qualitatively evaluate the model's performance by generating sample text from 
each of the model checkpoints that we test. 
