\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\newcommand{\authoremail}[2]{%
  \IEEEauthorblockN{#1}
  \IEEEauthorblockA{\textit{University of Alabama at Birmingham} \\ #2}
}

\title{GPT (to be changed)}

\author{\authoremail{Michael Gathara}{mikegtr@uab.edu}
\and
\authoremail{Yang (Jason) Liu}{yliu0602@uab.edu}
\and
\authoremail{Vaishak Menon}{vmenon19@uab.edu}
\and
\authoremail{Elisabeth Molen}{emolen@uab.edu}
\and
\authoremail{Trenton Davis}{trentd@uab.edu}
\and
\authoremail{Akshar Patel}{akshar2020@uab.edu}
}

\maketitle

\begin{abstract}
Large language models (LLMs) are a subset of artificial intelligence (AI)
models that have the ability to intake, understand, and generate human 
readable text. Since their inception, these models have found rapid success and widespread adoption in a vast range of industries and 
use cases including software engineering, healthcare, research, 
and supply-chain management. A specific type of LLM- generative pretrained 
transformers (GPTs)- stand out as one of the most used and adopted models as
they are tuned to be highly effective at understanding conversational 
context and generating coherent and relevant text. Built on top of the transformer architecture, GPTs are often more
finely tuned to perform specific tasks after
first being trained on a large corupus of less specific textual data- hence the pre-trained portion of the name. While the transformer has remained the 
core of these models, many different optimizations have been proposed and 
implemented over the past several years including flash attention and multi-layer attention (MLA). In this work, we implement a scratch-built GPT equipped with some 
of these optimizations, train it on the fineweb dataset, and then test 
it on several tasks and analyze some of the features of the model. 
\end{abstract}


\section{Introduction}
\input{sections/introduction.tex}

\section{Methods}
\input{sections/methods.tex}

\section{Results}
\input{sections/results.tex}

\section{Discussion}
\input{sections/discussion.tex}

\section{Conclusion}
\input{sections/conclusion.tex}

\break{}
\bibliographystyle{unsrt}
\bibliography{references} % .bib 

\end{document}
